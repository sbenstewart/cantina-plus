% Chapter 6

\chapter{Results} % Write in your own chapter title
\label{Chapter6}
\lhead{Chapter 6. \emph{Results}} % Write in your own chapter title to set the page header

This chapter provides the results of simulation runs for various standard topologies and compares it with the reconfigurable topology synthesized for an application. It also provides a theoretical study of the performance improvement provided by the FAA implementation for \textit{ext2} filesystem.

\section{Reconfigurable ICN}
\subsection{Existing System}
The existing many-core architectures have only standard interconnection topologies. The available interconnection networks are
\begin{itemize}
\item 2D Mesh
\item Torus
\item Hypercube
\item Butterfly
\item Omega Shuffle
\item 3D Mesh
\item Flat Tree
\end{itemize}

\subsection{Metrics Used}
       The Reconfigurable Interconnection network generated is analyzed for performance in Booksim. The performance measures analyzed are:
\begin{enumerate}
\item Average Latency
\item Hopcount
\end{enumerate}

\subsection{Results Comparison}
The R-ICN is compared with other standard Interconnection Networks like
Torus, Omega, 2D mesh, 3D mesh and the results are graphically presented in Figure \ref{fg:avglat} and \ref{fg:avghc}. It
has been found that R-ICN has lower latencies compared to other Interconnection
Networks.The average is obtained from the BookSim simulator which is
calculated by running the simulation six times. It can be found that the
Reconfigurable Interconnection Network generated by the algorithm will ensure
efficient and fast communication between the processors for a given communication
graph of an algorithm execution.

Figures \ref{fg:avglat} and \ref{fg:avghc} present the results obtained when the interconnection networks were tested over \textit{uniform traffic} generated by Booksim. It is to be noted that, 3D mesh has a lesser average hop count when compared to RICN synthesized. This is because uniform traffic was used for simulation and RICN is specific to an application's communication pattern which is not uniform.

\begin{figure}
\begin{center}
\includegraphics[scale=1]{avglatency.png}
 \caption{Analysis of various ICNs and Average Latencies}
 \label{fg:avglat}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[scale=1]{avghopcount.png}
 \caption{Analysis of various ICNs and Average Hopcount}
 \label{fg:avghc}
\end{center}
\end{figure}

Figures \ref{fg:RTG1} and \ref{fg:RTG2} are the graphs representing the results when the ICNs were simulated over \textit{real traffic} pattern. The applications used for analysis were bzip compression, and Optimized Laplace Solver on a 2D Grid of HOMB \cite{laplace}.

It can be seen in all the cases that, the synthesised RICN outperforms all the other network topologies when simulated with the application's trace file. Simulation results clearly show that synthesised RICNs perform better for lower number of nodes (homb 10 and homb 32). This implies that the synthesis algorithm is not highly scalable.

\begin{figure}
\begin{center}
\includegraphics[scale=0.65]{real_avg_l_pat.png}
 \caption{Average Latency of various ICNs over Real Traffic Pattern}
 \label{fg:RTG1}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.65]{real_avg_h_pat.png}
 \caption{Average Hops of various ICNs over Real Traffic Pattern}
 \label{fg:RTG2}
\end{center}
\end{figure}

\section{FAA Theoretical Performance Analysis}
\label{sec:perf}
Performance of our model is measured based on the Time For Search($TFS$) metric. It represents the time taken to find a file/directory name match within a given directory.
\begin{eqnarray}
\label{eq:tfs}
 TFS = T_{h} + T_{f} + (T_{m} + T_{p}) * d
\end{eqnarray}
where $T_{h}$ is the time to perform a hash computation by the HU, $T_{f}$ is the time taken for a block read from the flash memory, $T_{m}$ is the time taken to load the internal register buffer from the block RAMs, $T_{p}$ is the time taken to compare a single filename and $d$ is the average number of directory entries needed to be searched before locating the target file.
\begin{eqnarray}
\label{eq:tm}
T_{m} = \Big( \frac{K}{N*M} \Big) * \frac{C_{m}}{F} \\
\label{eq:tp}
T_{p} = \Big\lceil \frac{AVG(F_{L})}{K} \Big\rceil * \frac{C_{c}}{F}
\end{eqnarray}
where $K$ is the size of the register buffer and comparator (in bytes), $N$ is the number of block RAMs, $M$ is the number of read/write ports in a block RAM, $AVG(F_{L})$ is the average filename length (in bytes), $C_{m}$ is the number of cycles required by a single block RAM to fetch $\frac{K}{N*M}$ bytes and $C_{c}$ is the number of cycles taken by the comparator to complete a comparison operation. $F$ the clock frequency is the same for both the comparison as well as the memory access operations from block RAM \cite{bram}.

From equation (\ref{eq:tfs}), it can be seen that the one-time investment of the $T_{f} + T_{h}$ can be sufficiently hidden by the time taken to fetch and search the directory entries using read ahead techniques. Further, the memory access time for $K$ bytes from RAM is reduced by a factor of $N*M$ using this parallel memory fetch operation. The characteristic equation (\ref{eq:tfs}) of the FAA reveal a linear relationship between number of directory entries and time required to do the search.

For example, if $T_{f}= 30\mu s$ (for a 4KB flash block obtained from \cite{flash,intel,sam}), $T_{h}= 5$ cycles, $K=32$ bytes, $N=4$, $M=2$, $C_{m}=1$ clock cycle for a 32 bits (4 bytes) transfer (for Xilinx's Block RAM \cite{bram}), $F=500MHz$, $C_{c}=1$ (for Xilinx Virtex 4 FPGA Series \cite{bram}) with $d=100K$ and $AVG(F_{L})=64$ bytes (from \cite{ext2doc}), the parameters $T_{h}+T_{f}$ is around 30$\mu s$ and $(T_{m} + T_{p})$ is about $12ns$ approximately. Thus, for a single directory search, $TFS=1.1ms$ for a directory size of $10^5$ directory entries and with an average filename length of 64 bytes. This when compared to the traditional software implementation that utilize HDD gives better performance as the latter involves disk read latencies (which itself is in the order of milliseconds apart from delay due to search).


In conclusion, this chapter discusses about the performance improvement achieved by exploiting reconfigurability in both interconnection networks and filesystems. The report is concluded in the following chapter with the contributions of this project and also hinting on possible future work.

